{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "from utils.ModelWrapper import ModelWrapper, ModelType\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import wandb\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdat550\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\henriks\\Documents\\skole\\master_linje\\2_semester\\DAT550\\prosjekt\\DAT550-project\\wandb\\run-20220514_133128-1qind4gd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dat550/deepfake-basic/runs/1qind4gd\" target=\"_blank\">efficient-cloud-176</a></strong> to <a href=\"https://wandb.ai/dat550/deepfake-basic\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dat550/deepfake-basic/runs/1qind4gd?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x280a0ae79a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init\n",
    "wandb_project_name = 'deepfake-basic'\n",
    "data_dir = \"./data/data-large/data\"\n",
    "img_size = 128\n",
    "wandb.init(project=wandb_project_name, entity=\"dat550\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wandb Sweep training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "   # Default values for hyper-parameters we are going to sweep over\n",
    "   config_defaults = {\n",
    "         'epochs': 1,\n",
    "         'batch_size': 32,\n",
    "         'learning_rate': 0.0001,\n",
    "         'optimizer': 'adam',\n",
    "         'hidden_layer_size': 64,\n",
    "         'conv_layer_1_size': 16,\n",
    "         'conv_layer_2_size': 32,\n",
    "         'conv_layer_3_size': 64,\n",
    "         'dropout': 0.5,\n",
    "         \"use_augmentation\": True,\n",
    "      }\n",
    "   # Initialize wandb with a sample project name\n",
    "   wandb.init(config=config_defaults, project=wandb_project_name, entity=\"dat550\")\n",
    "\n",
    "   # # Config is a variable that holds and saves hyperparameters and inputs\n",
    "   config = wandb.config\n",
    "\n",
    "   # init wrapper\n",
    "   modeltype = ModelType.CNN\n",
    "   model_wrapper = ModelWrapper(data_dir, img_size, config, modeltype=modeltype,use_wandb=False, use_generator=False, sample_datasets=True)\n",
    "\n",
    "   # Define the optimizer\n",
    "   if config.optimizer=='sgd':\n",
    "      optimizer = tf.keras.optimizers.SGD(learning_rate=config.learning_rate) \n",
    "   elif config.optimizer=='rmsprop':\n",
    "      optimizer = tf.keras.optimizers.RMSprop(learning_rate=config.learning_rate,)\n",
    "   elif config.optimizer=='adam':\n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate,) \n",
    "   elif config.optimizer=='nadam':\n",
    "      optimizer = tf.keras.optimizers.Nadam(learning_rate=config.learning_rate,) \n",
    "   \n",
    "   # Compile the model\n",
    "   model_wrapper.model.compile(optimizer=optimizer,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "   # Train the model\n",
    "   model_wrapper.fit()\n",
    "   return model_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep training\n",
    "sweep_config = {\n",
    "    \"name\": \"deepfake-basic-sweep\",\n",
    "   'method': 'bayes',\n",
    "    \"metric\": {\n",
    "            \"name\": \"accuracy\",\n",
    "            \"goal\": \"maximize\"\n",
    "        },\n",
    "    \"early_terminate\": {\n",
    "       \"type\": \"hyperband\",\n",
    "       \"min_iter\": 5\n",
    "   },\n",
    "   'parameters': {\n",
    "        'batch_size': {\n",
    "            'values': [64, 128, 256] \n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.0, 0.2, 0.5]\n",
    "        },\n",
    "        'conv_layer_1_size': {\n",
    "            'values': [16,32], #32\n",
    "        },\n",
    "        'conv_layer_2_size': {\n",
    "            'values': [32,64] #64\n",
    "        },\n",
    "        'conv_layer_3_size': {\n",
    "            'values': [64 ,128] # 128\n",
    "        },\n",
    "        'hidden_layer_size': {\n",
    "                'values': [64, 128] \n",
    "            },\n",
    "      'learning_rate': {\n",
    "            'values': [1e-2, 1e-3, 1e-4, 3e-4]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam',  \"nadam\", 'sgd', 'rmsprop']\n",
    "        },\n",
    "          \"use_augmentation\": {\n",
    "            \"values\": [False, True] \n",
    "        }\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1qind4gd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e0882649a94364b826ee2305ff938f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">efficient-cloud-176</strong>: <a href=\"https://wandb.ai/dat550/deepfake-basic/runs/1qind4gd\" target=\"_blank\">https://wandb.ai/dat550/deepfake-basic/runs/1qind4gd</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220514_133128-1qind4gd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1qind4gd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\henriks\\Documents\\skole\\master_linje\\2_semester\\DAT550\\prosjekt\\DAT550-project\\wandb\\run-20220514_133136-2ijnaa3r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dat550/deepfake-basic/runs/2ijnaa3r\" target=\"_blank\">colorful-feather-177</a></strong> to <a href=\"https://wandb.ai/dat550/deepfake-basic\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29988 files belonging to 2 classes.\n",
      "Using 5997 files for validation.\n",
      "Found 3431 files belonging to 2 classes.\n",
      "Using 686 files for validation.\n",
      "Found 1587 files belonging to 2 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "188/188 [==============================] - 59s 294ms/step - loss: 2.7939 - accuracy: 0.5121 - val_loss: 0.6930 - val_accuracy: 0.5262\n"
     ]
    }
   ],
   "source": [
    "USE_SWEEP = False\n",
    "if USE_SWEEP:\n",
    "    sweep_id = wandb.sweep(sweep_config, project=wandb_project_name, entity=\"dat550\")\n",
    "    wandb.agent(sweep_id, function=train)\n",
    "else:\n",
    "    model_wrapper = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_wandb_model = False\n",
    "if eval_wandb_model:\n",
    "    # load the bess model NB: edit namd and path\n",
    "    loaded_model = wandb.restore('model-best.h5', run_path=\"dat550/deepfake-efficientnet/runs/29425ckm\")\n",
    "    model_to_eval = load_model(loaded_model.name)\n",
    "elif model_wrapper != None:\n",
    "    model_to_eval = model_wrapper.model\n",
    "else:\n",
    "    raise Exception(\"No model to evaluate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 4s 75ms/step\n",
      "self.test_data.file_paths:  1587\n",
      "preds:  1587\n",
      "conf_matrix: \n",
      " [[  0   0]\n",
      " [777 810]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_wrapper.evaluate_model(model_to_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper.export_to_png()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6186f98b4107da167f8e04d2b53209365f80479d5668ca57b5633229f74e4af5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
