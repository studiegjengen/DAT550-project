{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full flow: preprocess, train and predict\n",
    "Full flow of all modules implemented. Change `modeltype` to test differens models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import preprocess_videos\n",
    "from utils.ModelWrapper import ModelWrapper, ModelType\n",
    "import tensorflow as tf\n",
    "from utils.predict import predict_multiple\n",
    "from utils.dot_dict import Dotdict\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init configuration\n",
    "Change the configuration here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WANDB = True\n",
    "MODEL_TYPE = ModelType.CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train datasets\n",
      "\t Processing train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [03:06, 16.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test datasets\n",
      "\t Processing test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [03:07, 18.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# takes approx 6 min\n",
    "datasets = {\n",
    "    \"train\": [\"train\"], # Train dataset folders, now points to sample files\n",
    "    \"test\": [\"test\"], # Test dataset fodler, not points to sample files\n",
    "}\n",
    "\n",
    "# When processing full directories remove skip_sampling=True because we don't need 50/50 split in the example.\n",
    "preprocess_videos(datasets, save_dir=\"data\", video_dir=\"sample\", skip_sampling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdat550\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\henriks\\Documents\\skole\\master_linje\\2_semester\\DAT550\\prosjekt\\DAT550-project\\wandb\\run-20220514_135558-y85278nl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dat550/deepfake-efficientnet/runs/y85278nl\" target=\"_blank\">glowing-vortex-36</a></strong> to <a href=\"https://wandb.ai/dat550/deepfake-efficientnet\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configs\n",
    "# config_defaults = {\n",
    "#     'epochs': 3,\n",
    "#     'batch_size': 32,\n",
    "#     'learning_rate': 0.0001,\n",
    "#     'dropout': 0.5,\n",
    "#     'regularization': 0.0001,\n",
    "# }\n",
    "\n",
    "config_defaults = {\n",
    "    'epochs': 1,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.0001,\n",
    "    'optimizer': 'adam',\n",
    "    'hidden_layer_size': 64,\n",
    "    'conv_layer_1_size': 16,\n",
    "    'conv_layer_2_size': 32,\n",
    "    'conv_layer_3_size': 64,\n",
    "    'dropout': 0.5,\n",
    "    \"use_augmentation\": True,\n",
    "}\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.init(config=config_defaults, project=\"deepfake-efficientnet\", entity=\"dat550\")\n",
    "    config = wandb.config\n",
    "else:\n",
    "    config = Dotdict(config_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 128\n",
    "data_dir = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 66 images belonging to 2 classes.\n",
      "Found 8 images belonging to 2 classes.\n",
      "Found 159 images belonging to 2 classes.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The data_type argument of wandb.keras.WandbCallback is deprecated and will be removed in a future release. Please use input_type instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Setting input_type = data_type.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 128, 128, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 64, 64, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 32, 32, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1048640   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,072,289\n",
      "Trainable params: 1,072,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model_file = f'models/{MODEL_TYPE.name}/{wandb.run.name}_model.h5'\n",
    "\n",
    "# init wrapper\n",
    "model_wrapper = ModelWrapper(data_dir, img_size, config,model_file=model_file, modeltype=MODEL_TYPE, use_wandb=USE_WANDB)\n",
    "model_wrapper.model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_wrapper.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause has no variant outputs and has no vectorized variant inputs\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause has no variant outputs and has no vectorized variant inputs\n",
      "1/1 [==============================] - 0s 196ms/steps: 0.6805 - accuracy: 0.62\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.63623, saving model to models/CNN\\glowing-vortex-36_model.h5\n",
      "3/3 [==============================] - 7s 1s/step - loss: 0.6813 - accuracy: 0.6212 - val_loss: 0.6362 - val_accuracy: 0.6250 - _timestamp: 1652529376.0000 - _runtime: 18.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d902493af0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 1s 7ms/step\n",
      "self.test_generator.filenames:  159\n",
      "preds:  159\n",
      "conf_matrix: \n",
      " [[140  19]\n",
      " [  0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD4CAYAAADB0SsLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAIl0lEQVR4nO3abWyVZx3H8d9/liXMCWwInbSVVIY6tjpcAhp0GeoojIWh6NxGpslEERfUhc0Nx8PidIDEjBBdwCodmwGm0b2ATQcJiXbysEFUGA+aMYZrCyuEUdB1BKSXb5oGxukpB9be5Xe+n6Qv7us+J/e/ufLNffecRkpJADxdkvUAALoOgQPGCBwwRuCAMQIHjJV09QWmRR8+pr/ILNlbl/UIKFBUDo9c69zBAWMEDhgjcMAYgQPGCBwwRuCAMQIHjBE4YIzAAWMEDhgjcMAYgQPGCBwwRuCAMQIHjBE4YIzAAWMEDhgjcMAYgQPGCBwwRuCAMQIHjBE4YIzAAWMEDhgjcMAYgQPGCBwwRuCAMQIHjBE4YIzAAWMEDhgjcMAYgQPGCBwwRuCAMQIHjBE4YIzAAWMEDhgjcMAYgQPGCBwwRuCAMQIHjBE4YIzAAWMEDhgj8A58bdkTWtj0mua8svmsczfPmK6l6Zje3//K9rWvLl6oR1/9h2Zv26iKT17fnaMih4cfX6JRd3xLE759f/vaP/fu0x33zdaEaQ9o2iM/1X/fbslwwu5B4B3YtHyFfj5u0lnrV5SX6ZrqL+jwv99oX7vulmoNHDpEc4cO14qp39fkJYu6c1Tk8KUxN+lXP/nhGWuzF/1S939jstYs/ZnGjBqpZb9fk9F03YfAO7DnxY1qeevIWeu3L5qvZx+cI6XUvvaJieO1+elVkqTXX9qi3v36qs9Vpd02K842omqY+n7g8jPW9jUe0IiqayRJo26o0roNL2UxWrcq6ewFEfFxSRMllbUtNUpanVLa3ZWD9UTX3zZezY0H1Lh9xxnr/coG6Uh9Q/txc0Oj+pUN0rE3m7p7RORx9eAKrd+0VTePGqEX6jbrwKHDWY/U5fLewSPiIUnPSApJL7f9hKRVETEzz/umRsTWiNi6Syfey3kz06t3b417+AGtnvtY1qPgPM2bMU0rn1unSdNn6u133lGvkk7vbxe9zn7DKZKuTSmdPH0xIh6XtFPSglxvSinVSKqRpGnRJ+V6zcVmwJBK9a8crDnbNkiS+pWXadbfXtSCkZ9Tc+N+XVFR3v7afuVlam7cn9Wo6MBHKspUO2+WJOn1hv36y8t/z3iirtfZ3+CtkgblWP9Q27misX/HLj1YOkSzKqs0q7JKzQ2NeuyGG3Ws6aC2r/6TPv31uyRJlZ8aoeNHj/F43gMdbj4qSWptbdXSVc/qzlvHZDxR1+vsDn6fpPUR8aqk+ra1D0u6WtL0Lpwrc1NW1uqjoz+ryz/YX/Prd2vNI/O0sfY3OV+7449rdd34av14zzadaGnRU/fc283T4t1mzF+sLdt36cix/+imu7+j7959u1qOH9eKNeskSdWfGalJ1aOzHbIbREr5n6Aj4hJJI3Xmh2xbUkqnzuUCLo/oxWTJ3rqsR0CBonJ45Frv9FOGlFKrpLP/2wNAj8f34IAxAgeMEThgjMABYwQOGCNwwBiBA8YIHDBG4IAxAgeMEThgjMABYwQOGCNwwBiBA8YIHDBG4IAxAgeMEThgjMABYwQOGCNwwBiBA8YIHDBG4IAxAgeMEThgjMABYwQOGCNwwBiBA8YIHDBG4IAxAgeMEThgjMABYwQOGCNwwBiBA8YIHDBG4IAxAgeMEThgjMABYwQOGCNwwBiBA8YIHDAWKaWuvULL0S6+AABd1jdyLXMHB4wROGCMwAFjBA4YI3DAGIEDxggcMEbggDECB4wROGCMwAFjBA4YI3DAGIEDxggcMEbggDECB4wROGCMwAFjBA4YI3DAGIEDxggcMEbggDECB4wROGCMwAFjBA4YI3DAGIEDxggcMEbggDECB4wROGCMwAFjBA4YI3DAGIEDxggcMEbggDECB4wROGCMwAFjBA4YI3DAGIEDxggcMEbggDECB4wR+Hmo27BJY7/4FY25bZJqap/Kehycg2LdMwIv0KlTp/TogoX69S8W6/k//FbPvbBWe17bm/VYyKOY94zAC7R9x04NrihXRXmZLu3VS7eOrdb6P9dlPRbyKOY9I/ACNR08pKtKS9uPS0sHqunQoQwnQmeKec/OO/CIuCfPuakRsTUittbULj/fSwC4QCUX8N4fSXoy14mUUo2kGklSy9F0AdfocUoHDtCbTU3tx01NB1U6YECGE6Ezxbxnee/gEbG9g59XJJXme6+rqmuHad8b9apvbNSJkyf1/Np1+vzoG7MeC3kU8551dgcvlTRW0pF3rYekjV0yUQ9XUlKiuQ/9QN+893s61dqqL0+coKFDhmQ9FvIo5j2LlDp+go6IZZKeTCn9Nce5lSmlyZ1ewewRHeiRLusbuZbzBv6eIHCg63UQOF+TAcYIHDBG4IAxAgeMEThgjMABYwQOGCNwwBiBA8YIHDBG4IAxAgeMEThgjMABYwQOGCNwwBiBA8YIHDBG4IAxAgeMEThgjMABYwQOGCNwwBiBA8YIHDBG4IAxAgeMEThgjMABYwQOGCNwwBiBA8YIHDBG4IAxAgeMEThgjMABYwQOGCNwwBiBA8YIHDBG4IAxAgeMEThgjMABYwQOGCNwwBiBA8YipZT1DBetiJiaUqrJeg6cm2LcL+7gF2Zq1gOgIEW3XwQOGCNwwBiBX5ii+nvOQNHtFx+yAca4gwPGCBwwRuDnISLGRcS/ImJPRMzMeh7kFxG1EXEwInZkPUt3I/ACRcT7JD0h6RZJwyTdFRHDsp0KnVguaVzWQ2SBwAs3UtKelNLelNIJSc9ImpjxTMgjpVQn6a2s58gCgReuTFL9accNbWtAj0PggDECL1yjpIrTjsvb1oAeh8ALt0XS0IiojIhLJd0paXXGMwE5EXiBUkr/kzRd0lpJuyX9LqW0M9upkE9ErJK0SdLHIqIhIqZkPVN34V9VAWPcwQFjBA4YI3DAGIEDxggcMEbggDECB4z9H0v40x9IxGZ9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_to_eval = model_wrapper.model\n",
    "model_wrapper.evaluate_model(model_to_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict_multiple(\"./sample/test\", model_to_eval, frame_rate=1, plot=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6186f98b4107da167f8e04d2b53209365f80479d5668ca57b5633229f74e4af5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
